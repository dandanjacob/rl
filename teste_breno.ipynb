{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Daniel\\Desktop\\Assault\\rl\\teste_breno.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Desktop/Assault/rl/teste_breno.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense, Flatten, Convolution2D\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Desktop/Assault/rl/teste_breno.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Desktop/Assault/rl/teste_breno.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m \u001b[39mimport\u001b[39;00m DQNAgent\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Desktop/Assault/rl/teste_breno.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmemory\u001b[39;00m \u001b[39mimport\u001b[39;00m SequentialMemory\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniel/Desktop/Assault/rl/teste_breno.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolicy\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearAnnealedPolicy, EpsGreedyQPolicy\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rl'"
     ]
    }
   ],
   "source": [
    "#importação das bibliotecas necessárias\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificação de ambientação\n",
    "# Verifica se o ambiente está registrado\n",
    "keys = gym.envs.registry.keys()\n",
    "print(\"ALE/Assault-v5\" in keys)\n",
    "\n",
    "# Cria o ambiente\n",
    "env = gym.make(\"ALE/Assault-v5\", render_mode=\"rgb_array\", obs_type=\"rgb\", full_action_space=False)\n",
    "\n",
    "# Verifica o espaço de observação e ações\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "print(f\"Height: {height}, Width: {width}, Channels: {channels}\")\n",
    "print(f\"Actions: {actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aleatório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste de execução\n",
    "from copy import deepcopy\n",
    "EPISODES = 1\n",
    "for episode in range(1, EPISODES + 1):\n",
    "    state = env.reset()\n",
    "    DONE = False\n",
    "    SCORE = 0\n",
    "\n",
    "    # Loop de execução\n",
    "    while not DONE:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, DONE, TRUNCATED, info = env.step(action)\n",
    "        SCORE += reward\n",
    "\n",
    "    # Exibe o resultado do episódio\n",
    "    print(f\"Episode: {episode}/{EPISODES}, Score: {SCORE}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    #model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1.0, value_min=0.1, value_test=0.05, nb_steps=1000000)\n",
    "    memory = SequentialMemory(limit=1000000, window_length=4)\n",
    "    dqn = DQNAgent(model=model, policy=policy, nb_actions=actions, memory=memory, nb_steps_warmup=50000, target_model_update=10000)\n",
    "\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-4), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps=10000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=5, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assault",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
